{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "nteract": {
      "version": "0.14.5"
    },
    "colab": {
      "name": "LS_DS_Unit_4_Sprint_2_Challenge.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JefNtungila/DS-Unit-4-Sprint-2-Neural-Networks/blob/master/LS_DS_Unit_4_Sprint_2_Challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KviD5cCQ9081",
        "colab_type": "text"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "## *Data Science Unit 4 Sprint 2*\n",
        "\n",
        "# Sprint Challenge - Neural Network Foundations\n",
        "\n",
        "Table of Problems\n",
        "\n",
        "1. [Defining Neural Networks](#Q1)\n",
        "2. [Perceptron on XOR Gates](#Q2)\n",
        "3. [Multilayer Perceptron](#Q3)\n",
        "4. [Keras MMP](#Q4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAO1y81t9082",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"Q1\"></a>\n",
        "## 1. Define the following terms:\n",
        "\n",
        "- **Neuron:** \n",
        "- **Input Layer:** \n",
        "- **Hidden Layer:** \n",
        "- **Output Layer:**\n",
        "- **Activation:**\n",
        "- **Backpropagation:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlaZJ_VQAgPS",
        "colab_type": "text"
      },
      "source": [
        "### 1 -  A node connecting the information input to the output information can be considered a  neuron\n",
        "### 2 - The input layer is the layer that gets the data into the hidden layer\n",
        "### 3 - The hidden layer consists of one or multiple layers of nodes/ neuros\n",
        "### 4 - The output layer returns the processed result. \n",
        "### 5 - Activation helps a model learn. Perceptron are either 0 and 1. Activation adds weights and hence more control.\n",
        "### 6 - Backpropagation is the process of calculating the gradient of the error.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evJYbp1Q9083",
        "colab_type": "text"
      },
      "source": [
        "## 2. Perceptron on XOR Gates <a id=\"Q2\"></a>\n",
        "\n",
        "The XOr, or “exclusive or”, problem is a classic problem in ANN research. It is the problem of using a neural network to predict the outputs of XOr logic gates given two binary inputs. An XOr function should return a true value if the two inputs are not equal and a false value if they are equal. Create a perceptron class that can model the behavior of an AND gate. You can use the following table as your training data:\n",
        "\n",
        "|x1\t|x2 | y |\n",
        "|---|---|---|\n",
        "| 0 | 0 | 0 |\n",
        "| 0 | 1 | 1 |\n",
        "| 1 | 1 | 0 |\n",
        "| 1 | 0 | 1 |\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "inputHidden": false,
        "outputHidden": false,
        "id": "avrud7G79084",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "outputId": "2bf1d54f-7357-4f1c-ac2d-a202cbafa5d7"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "x1 = [0, 0, 1, 1]\n",
        "x2 = [0, 1, 1, 0]\n",
        "y = [0 , 1, 0, 1]\n",
        "predictions = np.add(x1, x2)\n",
        "difference = np.subtract(predictions, y)\n",
        "difference_squared = np.square(difference)\n",
        "\n",
        "df = pd.DataFrame({'x1': x1, 'x2': x2, \"y\": y, \"predictions\": predictions, \n",
        "                   \"difference\": difference, \"diff_squared\": difference_squared})\n",
        "df.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x1</th>\n",
              "      <th>x2</th>\n",
              "      <th>y</th>\n",
              "      <th>predictions</th>\n",
              "      <th>difference</th>\n",
              "      <th>diff_squared</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   x1  x2  y  predictions  difference  diff_squared\n",
              "0   0   0  0            0           0             0\n",
              "1   0   1  1            1           0             0\n",
              "2   1   1  0            2           2             4\n",
              "3   1   0  1            1           0             0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWrlB5909086",
        "colab_type": "text"
      },
      "source": [
        "## 3. Multilayer Perceptron <a id=\"Q3\"></a>\n",
        "\n",
        "Implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights.\n",
        "Your network must have one hidden layer.\n",
        "You do not have to update weights via gradient descent. You can use something like the derivative of the sigmoid function to update weights.\n",
        "Train your model on the Heart Disease dataset from UCI:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "inputHidden": false,
        "outputHidden": false,
        "id": "0H3H8H2E9086",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFAf8gprRGzA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "2f980b6f-9181-4e08-d78b-fbaa2201a1d0"
      },
      "source": [
        "df.columns.values"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg',\n",
              "       'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDZSIkYnQZmg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "47c3137f-ab5d-4156-e580-51bb9fa0fb8f"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df.drop('target', axis = 1)\n",
        "y = df['target']\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape) \n",
        "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.33, random_state=42)\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(203, 13) (100, 13) (203,) (100,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtayVO6zdVFa",
        "colab_type": "text"
      },
      "source": [
        "###training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXZFWf1cRcdO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "e955566b-4abe-424a-c8c0-2a9da83c23ef"
      },
      "source": [
        " \n",
        "import numpy as np\n",
        "\n",
        "class NeuralNetwork():\n",
        "    \n",
        "    def __init__(self):\n",
        "        # seeding for random number generation\n",
        "        np.random.seed(1)\n",
        "        \n",
        "        #converting weights to a 3 by 1 matrix with values from -1 to 1 and mean of 0\n",
        "        self.synaptic_weights = 2 * np.random.random((13, 203)) - 1\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        #applying the sigmoid function\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        #computing derivative to the Sigmoid function\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def train(self, training_inputs, training_outputs, training_iterations):\n",
        "        \n",
        "        #training the model to make accurate predictions while adjusting weights continually\n",
        "        for iteration in range(training_iterations):\n",
        "            #siphon the training data via  the neuron\n",
        "            output = self.think(training_inputs)\n",
        "\n",
        "            #computing error rate for back-propagation\n",
        "            error = training_outputs - output\n",
        "            \n",
        "            #performing weight adjustments\n",
        "            adjustments = np.dot(training_inputs.T, error * self.sigmoid_derivative(output))\n",
        "\n",
        "            self.synaptic_weights += adjustments\n",
        "\n",
        "    def think(self, inputs):\n",
        "        #passing the inputs via the neuron to get output   \n",
        "        #converting values to floats\n",
        "        \n",
        "        inputs = inputs.astype(float)\n",
        "        output = self.sigmoid(np.dot(inputs, self.synaptic_weights))\n",
        "        return output\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    #initializing the neuron class\n",
        "    neural_network = NeuralNetwork()\n",
        "\n",
        "    print(\"Beginning Randomly Generated Weights: \")\n",
        "    print(neural_network.synaptic_weights)\n",
        "\n",
        "    #training data consisting of 4 examples--3 input values and 1 output\n",
        "    training_inputs = X_train.to_numpy()\n",
        "\n",
        "    training_outputs = y_train.to_numpy()\n",
        "\n",
        "    #training taking place\n",
        "    neural_network.train(training_inputs, training_outputs, 15000)\n",
        "\n",
        "    print(\"Ending Weights After Training: \")\n",
        "    print(neural_network.synaptic_weights)\n"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Beginning Randomly Generated Weights: \n",
            "[[-0.16595599  0.44064899 -0.99977125 ...  0.90035224  0.11330638\n",
            "   0.8312127 ]\n",
            " [ 0.28313242 -0.21998457 -0.02801867 ...  0.4187745  -0.06999704\n",
            "   0.89509788]\n",
            " [-0.55713453 -0.46585596 -0.83705207 ...  0.42262904  0.10776922\n",
            "  -0.39096402]\n",
            " ...\n",
            " [-0.43013571  0.5377057   0.28156541 ...  0.7405159   0.09393662\n",
            "   0.0863281 ]\n",
            " [ 0.23145186 -0.395637   -0.26653604 ... -0.58380518  0.49099666\n",
            "  -0.19318674]\n",
            " [-0.03105988  0.77491016 -0.7251129  ...  0.89126857 -0.23217152\n",
            "  -0.62207481]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: RuntimeWarning: overflow encountered in exp\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Ending Weights After Training: \n",
            "[[-0.16595599  0.44064899 -0.99977125 ...  0.90035224  0.11330364\n",
            "   0.8312127 ]\n",
            " [ 0.28313242 -0.21998457 -0.02801867 ...  0.4187745  -0.06999704\n",
            "   0.89509788]\n",
            " [-0.55713453 -0.46585596 -0.83705207 ...  0.42262904  0.10776914\n",
            "  -0.39096402]\n",
            " ...\n",
            " [-0.43013571  0.5377057   0.28156541 ...  0.7405159   0.09393658\n",
            "   0.0863281 ]\n",
            " [ 0.23145186 -0.395637   -0.26653604 ... -0.58380518  0.49099666\n",
            "  -0.19318674]\n",
            " [-0.03105988  0.77491016 -0.7251129  ...  0.89126857 -0.23217164\n",
            "  -0.62207481]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bddM8y0FdX_s",
        "colab_type": "text"
      },
      "source": [
        "###testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8y1gTwBZdZr2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "14332e90-6e6d-4239-bc12-6bfe12fcb10b"
      },
      "source": [
        "print(neural_network.think(y[0]))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.45860596 0.60841366 0.2689864  ... 0.71102188 0.52829565 0.69661129]\n",
            " [0.57031401 0.44522458 0.49299579 ... 0.60318996 0.48250788 0.70994108]\n",
            " [0.36421073 0.38559755 0.30215602 ... 0.60411218 0.52691624 0.40348525]\n",
            " ...\n",
            " [0.39409393 0.63127854 0.56992996 ... 0.67710866 0.52346689 0.52156863]\n",
            " [0.55760603 0.40236105 0.43375769 ... 0.35805749 0.62034119 0.45185296]\n",
            " [0.49223565 0.6845821  0.32626808 ... 0.70915189 0.44221642 0.34930971]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAp8tKqX9088",
        "colab_type": "text"
      },
      "source": [
        "## 4. Keras MMP <a id=\"Q4\"></a>\n",
        "\n",
        "Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy.\n",
        "Use the Heart Disease Dataset (binary classification)\n",
        "Use an appropriate loss function for a binary classification task\n",
        "Use an appropriate activation function on the final layer of your network.\n",
        "Train your model using verbose output for ease of grading.\n",
        "Use GridSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
        "When hyperparameter tuning, show you work by adding code cells for each new experiment.\n",
        "Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
        "You must hyperparameter tune at least 5 parameters in order to get a 3 on this section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4mCuAkEVHTf",
        "colab_type": "text"
      },
      "source": [
        "###Baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "inputHidden": false,
        "outputHidden": false,
        "id": "0qY7Rv329089",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import pandas as pd\n",
        "import numpy\n",
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Owx43LVZTqoV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "5fd5d7a2-bae9-4822-8462-7468ec0b6ef4"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "print(X)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.9521966   0.68100522  1.97312292 ... -2.27457861 -0.71442887\n",
            "  -2.14887271]\n",
            " [-1.91531289  0.68100522  1.00257707 ... -2.27457861 -0.71442887\n",
            "  -0.51292188]\n",
            " [-1.47415758 -1.46841752  0.03203122 ...  0.97635214 -0.71442887\n",
            "  -0.51292188]\n",
            " ...\n",
            " [ 1.50364073  0.68100522 -0.93851463 ... -0.64911323  1.24459328\n",
            "   1.12302895]\n",
            " [ 0.29046364  0.68100522 -0.93851463 ... -0.64911323  0.26508221\n",
            "   1.12302895]\n",
            " [ 0.29046364 -1.46841752  0.03203122 ... -0.64911323  0.26508221\n",
            "  -0.51292188]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFJ2_dQdT-gk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "446b167b-6a3c-441f-c777-4067c7145943"
      },
      "source": [
        "# Important Hyperparameters\n",
        "inputs = X.shape[1]\n",
        "epochs = 50\n",
        "batch_size = 10\n",
        "\n",
        "# Create Model\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation='relu', input_shape=(inputs,)))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "# Compile Model\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
        "# Fit Model\n",
        "model.fit(X, y, validation_split=0.33, epochs=epochs, batch_size=batch_size)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0816 16:45:22.067569 140693847697280 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 203 samples, validate on 100 samples\n",
            "Epoch 1/50\n",
            "203/203 [==============================] - 0s 2ms/sample - loss: 0.4645 - mean_squared_error: 0.4645 - val_loss: 0.9036 - val_mean_squared_error: 0.9036\n",
            "Epoch 2/50\n",
            "203/203 [==============================] - 0s 215us/sample - loss: 0.1392 - mean_squared_error: 0.1392 - val_loss: 0.3651 - val_mean_squared_error: 0.3651\n",
            "Epoch 3/50\n",
            "203/203 [==============================] - 0s 212us/sample - loss: 0.0999 - mean_squared_error: 0.0999 - val_loss: 0.4506 - val_mean_squared_error: 0.4506\n",
            "Epoch 4/50\n",
            "203/203 [==============================] - 0s 204us/sample - loss: 0.0827 - mean_squared_error: 0.0827 - val_loss: 0.3978 - val_mean_squared_error: 0.3978\n",
            "Epoch 5/50\n",
            "203/203 [==============================] - 0s 222us/sample - loss: 0.0708 - mean_squared_error: 0.0708 - val_loss: 0.4566 - val_mean_squared_error: 0.4566\n",
            "Epoch 6/50\n",
            "203/203 [==============================] - 0s 215us/sample - loss: 0.0642 - mean_squared_error: 0.0642 - val_loss: 0.4234 - val_mean_squared_error: 0.4234\n",
            "Epoch 7/50\n",
            "203/203 [==============================] - 0s 214us/sample - loss: 0.0563 - mean_squared_error: 0.0563 - val_loss: 0.4201 - val_mean_squared_error: 0.4201\n",
            "Epoch 8/50\n",
            "203/203 [==============================] - 0s 206us/sample - loss: 0.0511 - mean_squared_error: 0.0511 - val_loss: 0.4235 - val_mean_squared_error: 0.4235\n",
            "Epoch 9/50\n",
            "203/203 [==============================] - 0s 209us/sample - loss: 0.0450 - mean_squared_error: 0.0450 - val_loss: 0.4105 - val_mean_squared_error: 0.4105\n",
            "Epoch 10/50\n",
            "203/203 [==============================] - 0s 217us/sample - loss: 0.0417 - mean_squared_error: 0.0417 - val_loss: 0.3695 - val_mean_squared_error: 0.3695\n",
            "Epoch 11/50\n",
            "203/203 [==============================] - 0s 207us/sample - loss: 0.0375 - mean_squared_error: 0.0375 - val_loss: 0.4224 - val_mean_squared_error: 0.4224\n",
            "Epoch 12/50\n",
            "203/203 [==============================] - 0s 205us/sample - loss: 0.0346 - mean_squared_error: 0.0346 - val_loss: 0.3859 - val_mean_squared_error: 0.3859\n",
            "Epoch 13/50\n",
            "203/203 [==============================] - 0s 212us/sample - loss: 0.0320 - mean_squared_error: 0.0320 - val_loss: 0.3662 - val_mean_squared_error: 0.3662\n",
            "Epoch 14/50\n",
            "203/203 [==============================] - 0s 218us/sample - loss: 0.0285 - mean_squared_error: 0.0285 - val_loss: 0.4037 - val_mean_squared_error: 0.4037\n",
            "Epoch 15/50\n",
            "203/203 [==============================] - 0s 202us/sample - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.3775 - val_mean_squared_error: 0.3775\n",
            "Epoch 16/50\n",
            "203/203 [==============================] - 0s 185us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.3717 - val_mean_squared_error: 0.3717\n",
            "Epoch 17/50\n",
            "203/203 [==============================] - 0s 198us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.4129 - val_mean_squared_error: 0.4129\n",
            "Epoch 18/50\n",
            "203/203 [==============================] - 0s 211us/sample - loss: 0.0213 - mean_squared_error: 0.0213 - val_loss: 0.4053 - val_mean_squared_error: 0.4053\n",
            "Epoch 19/50\n",
            "203/203 [==============================] - 0s 229us/sample - loss: 0.0188 - mean_squared_error: 0.0188 - val_loss: 0.3988 - val_mean_squared_error: 0.3988\n",
            "Epoch 20/50\n",
            "203/203 [==============================] - 0s 203us/sample - loss: 0.0175 - mean_squared_error: 0.0175 - val_loss: 0.4200 - val_mean_squared_error: 0.4200\n",
            "Epoch 21/50\n",
            "203/203 [==============================] - 0s 210us/sample - loss: 0.0157 - mean_squared_error: 0.0157 - val_loss: 0.3760 - val_mean_squared_error: 0.3760\n",
            "Epoch 22/50\n",
            "203/203 [==============================] - 0s 220us/sample - loss: 0.0157 - mean_squared_error: 0.0157 - val_loss: 0.3874 - val_mean_squared_error: 0.3874\n",
            "Epoch 23/50\n",
            "203/203 [==============================] - 0s 216us/sample - loss: 0.0129 - mean_squared_error: 0.0129 - val_loss: 0.4036 - val_mean_squared_error: 0.4036\n",
            "Epoch 24/50\n",
            "203/203 [==============================] - 0s 200us/sample - loss: 0.0120 - mean_squared_error: 0.0120 - val_loss: 0.3637 - val_mean_squared_error: 0.3637\n",
            "Epoch 25/50\n",
            "203/203 [==============================] - 0s 194us/sample - loss: 0.0116 - mean_squared_error: 0.0116 - val_loss: 0.4029 - val_mean_squared_error: 0.4029\n",
            "Epoch 26/50\n",
            "203/203 [==============================] - 0s 204us/sample - loss: 0.0104 - mean_squared_error: 0.0104 - val_loss: 0.3753 - val_mean_squared_error: 0.3753\n",
            "Epoch 27/50\n",
            "203/203 [==============================] - 0s 213us/sample - loss: 0.0097 - mean_squared_error: 0.0097 - val_loss: 0.3859 - val_mean_squared_error: 0.3859\n",
            "Epoch 28/50\n",
            "203/203 [==============================] - 0s 190us/sample - loss: 0.0086 - mean_squared_error: 0.0086 - val_loss: 0.4107 - val_mean_squared_error: 0.4107\n",
            "Epoch 29/50\n",
            "203/203 [==============================] - 0s 200us/sample - loss: 0.0082 - mean_squared_error: 0.0082 - val_loss: 0.3998 - val_mean_squared_error: 0.3998\n",
            "Epoch 30/50\n",
            "203/203 [==============================] - 0s 208us/sample - loss: 0.0081 - mean_squared_error: 0.0081 - val_loss: 0.3813 - val_mean_squared_error: 0.3813\n",
            "Epoch 31/50\n",
            "203/203 [==============================] - 0s 201us/sample - loss: 0.0091 - mean_squared_error: 0.0091 - val_loss: 0.3840 - val_mean_squared_error: 0.3840\n",
            "Epoch 32/50\n",
            "203/203 [==============================] - 0s 206us/sample - loss: 0.0076 - mean_squared_error: 0.0076 - val_loss: 0.3848 - val_mean_squared_error: 0.3848\n",
            "Epoch 33/50\n",
            "203/203 [==============================] - 0s 189us/sample - loss: 0.0063 - mean_squared_error: 0.0063 - val_loss: 0.4033 - val_mean_squared_error: 0.4033\n",
            "Epoch 34/50\n",
            "203/203 [==============================] - 0s 215us/sample - loss: 0.0056 - mean_squared_error: 0.0056 - val_loss: 0.3974 - val_mean_squared_error: 0.3974\n",
            "Epoch 35/50\n",
            "203/203 [==============================] - 0s 196us/sample - loss: 0.0056 - mean_squared_error: 0.0056 - val_loss: 0.4031 - val_mean_squared_error: 0.4031\n",
            "Epoch 36/50\n",
            "203/203 [==============================] - 0s 196us/sample - loss: 0.0045 - mean_squared_error: 0.0045 - val_loss: 0.4204 - val_mean_squared_error: 0.4204\n",
            "Epoch 37/50\n",
            "203/203 [==============================] - 0s 203us/sample - loss: 0.0042 - mean_squared_error: 0.0042 - val_loss: 0.4157 - val_mean_squared_error: 0.4157\n",
            "Epoch 38/50\n",
            "203/203 [==============================] - 0s 205us/sample - loss: 0.0041 - mean_squared_error: 0.0041 - val_loss: 0.3905 - val_mean_squared_error: 0.3905\n",
            "Epoch 39/50\n",
            "203/203 [==============================] - 0s 196us/sample - loss: 0.0037 - mean_squared_error: 0.0037 - val_loss: 0.4130 - val_mean_squared_error: 0.4130\n",
            "Epoch 40/50\n",
            "203/203 [==============================] - 0s 210us/sample - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.4173 - val_mean_squared_error: 0.4173\n",
            "Epoch 41/50\n",
            "203/203 [==============================] - 0s 218us/sample - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.4287 - val_mean_squared_error: 0.4287\n",
            "Epoch 42/50\n",
            "203/203 [==============================] - 0s 202us/sample - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.4172 - val_mean_squared_error: 0.4172\n",
            "Epoch 43/50\n",
            "203/203 [==============================] - 0s 200us/sample - loss: 0.0035 - mean_squared_error: 0.0035 - val_loss: 0.4351 - val_mean_squared_error: 0.4351\n",
            "Epoch 44/50\n",
            "203/203 [==============================] - 0s 204us/sample - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.4167 - val_mean_squared_error: 0.4167\n",
            "Epoch 45/50\n",
            "203/203 [==============================] - 0s 217us/sample - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 0.4221 - val_mean_squared_error: 0.4221\n",
            "Epoch 46/50\n",
            "203/203 [==============================] - 0s 194us/sample - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.4120 - val_mean_squared_error: 0.4120\n",
            "Epoch 47/50\n",
            "203/203 [==============================] - 0s 208us/sample - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.4025 - val_mean_squared_error: 0.4025\n",
            "Epoch 48/50\n",
            "203/203 [==============================] - 0s 196us/sample - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.4049 - val_mean_squared_error: 0.4049\n",
            "Epoch 49/50\n",
            "203/203 [==============================] - 0s 195us/sample - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.4189 - val_mean_squared_error: 0.4189\n",
            "Epoch 50/50\n",
            "203/203 [==============================] - 0s 205us/sample - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.4235 - val_mean_squared_error: 0.4235\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff5941b4978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrGbgUdzWqYx",
        "colab_type": "text"
      },
      "source": [
        "### Parameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJjB8IHoW2OO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "outputId": "56129241-a77a-4a45-a198-cbf3de413998"
      },
      "source": [
        "import numpy\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "# # fix random seed for reproducibility\n",
        "# seed = 7\n",
        "# numpy.random.seed(seed)\n",
        "\n",
        "# # load dataset\n",
        "# url =\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "\n",
        "# dataset = pd.read_csv(url, header=None).values\n",
        "\n",
        "# # split into input (X) and output (Y) variables\n",
        "# X = dataset[:,0:8]\n",
        "# Y = dataset[:,8]\n",
        "\n",
        "# Function to create model, required for KerasClassifier\n",
        "def create_model():\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(12, input_dim=13, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    # Compile model\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "\n",
        "# define the grid search parameters\n",
        "# batch_size = [10, 20, 40, 60, 80, 100]\n",
        "# param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
        "\n",
        "# define the grid search parameters\n",
        "param_grid = {'batch_size': [10, 20, 40, 60, 80, 100],\n",
        "              'epochs': [20]}\n",
        "\n",
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
        "grid_result = grid.fit(X, y)\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") "
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best: 0.7161716222763062 using {'batch_size': 60, 'epochs': 20}\n",
            "Means: 0.6468646923700968, Stdev: 0.11835260513779586 with: {'batch_size': 10, 'epochs': 20}\n",
            "Means: 0.6534653604030609, Stdev: 0.1146122535976251 with: {'batch_size': 20, 'epochs': 20}\n",
            "Means: 0.6072607338428497, Stdev: 0.1346275748110329 with: {'batch_size': 40, 'epochs': 20}\n",
            "Means: 0.7161716222763062, Stdev: 0.09644646179303741 with: {'batch_size': 60, 'epochs': 20}\n",
            "Means: 0.3960396060720086, Stdev: 0.273788187979495 with: {'batch_size': 80, 'epochs': 20}\n",
            "Means: 0.48514851927757263, Stdev: 0.17729870427216968 with: {'batch_size': 100, 'epochs': 20}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}